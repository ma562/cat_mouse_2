<!DOCTYPE html>
<html lang="en" style = "scroll-behavior: smooth;">
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale = 1.0">
		<title>Cat & Mouse 2: Reinforcement Learning</title>
		<link rel="icon" href="images/jm.png">
		<link rel = "stylesheet" href="style.css">
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>

	<nav class = "navbar">
		<div class = "max-width">
			<div class = "logo"><a href="#">Cat & Mouse 2: Reinforcement Learning</a></div>
			<ul class = "menu">
				<li><a href="https://joseph-ma.com">Go Back</a></li>
			</ul>
		</div>
	</nav>

	<body>
		<div class = "top_info">
			<div class = "vid1">
				<iframe width="580" height="400" src="https://www.youtube.com/embed/Y97T-getn5I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</div>
			<div class = "information">
				<div class = "text">About</div>
				<p>
					Cat and Mouse 2: Reinforcement Learning builds upon the foundations of the original <b><a href = "https://ma562.github.io/joseph_ma-cat_mouse/" target = "_blank">Cat and Mouse Project</a></b>. Previously, the cat autonomously navigated a maze using <b><a href = "https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" target = "_blank">Dijkstra's algorithm</a></b> to chase a human-controlled mouse. In this sequel, we eliminate the human element by introducing a mouse driven by <b><a href = "https://en.wikipedia.org/wiki/Q-learning" target = "_blank">Q-learning</a></b>, a model-free <b><a href = "https://en.wikipedia.org/wiki/Reinforcement_learning" target = "_blank">reinforcement learning algorithm</a></b>.<br><br>

					In the new setup, once the mouse achieves near-perfect escape capabilities, its skills are further used to train a new, initially untrained cat. This cat is initially like a baby as it uses similar Q-learning techniques to adapt based on interactions with the expert mouse. Eventually, this newly trained cat outperforms the Dijkstra programmed cat due to its ability to understand the negative consequences of allowing the mouse to escape the maze. <br><br>

					The cycle continues: the newly trained cat then serves as a trainer for another new mouse. Through this generational training approach, each iteration refines their strategies, enhancing both the cat's and mouse's abilities to navigate the maze more effectively.<br>
				</p>
				<div class="button-container">
				    <a href="https://ma562.github.io/dijkstra-vs-q-learning-demo/" target = "_blank" class="learn-more-button">Dijkstra üê± vs Q1 üê≠</a>
				    <a href="https://ma562.github.io/dijkstra-vs-q2-learning-demo/" target = "_blank" class="learn-more-button">Dijkstra üê± vs Q2 üê≠</a>
				    <a href=https://ma562.github.io/q_learning1_vs_q_learning1_demo/ target = "_blank" class="learn-more-button">Q1 üê± vs Q2 üê≠</a>
				    <a href=https://ma562.github.io/q_learning2_vs_q_learning1_demo/ target = "_blank" class="learn-more-button">Q2 üê± vs Q1 üê≠</a>
				    <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ" target = "_blank" class="learn-more-button">Train your own mouse!</a>
				</div>
			</div>
		</div>
		<br>
		<div class = "mid_info">
			<div class = "algorithm">
				<div class = "text">MDP and Training Plan</div>
				Each training process can be modelled and described by a <b><a href = "https://en.wikipedia.org/wiki/Markov_decision_process" target = "_blank">Markov Decision Process</a></b> (MDP). The MDP is a mathematical framework used for modelling decision making. It typically consists of <b>States (S)</b> - which is a possible set of states or situations in which the agent (the player that is learning) can find itself in. <b>Actions (A)</b>- A set of actions that the agent can decide to perform. <b>Transition Function (P)</b>- A probability distribution that defines the likelihood of transitioning from one state to another, given a particular action. Since we are implementing Q-learning, the transition probabilities will not be explicitly modeled. <b>Reward Function (R)</b>- A function that provides immediate rewards based on the agent's actions and the state it transitions to. <b>Discount Factor (Œ≥)</b>- A discount that lessens the value of future rewards compared to immediate rewards. The longer we have to wait for a reward the less valuable it is. <br><br>
				The MDPs will differ depending on if we are training a cat or mouse since each agent has its own goals in the game. The training plan displayed shows the the agent towards the right. The player on the left can be seen as an "NPC" which is already an expert at what it does. Its goal is to teach an initially untrained agent to play the game well. While the theory suggests that agents should progressively improve with each training generation, practical challenges and limitations may arise, which we will explore further. 
			</div>
			<div class = "mdp_image">
				<img src = "images/mdp.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "dijkstra_info">
			<div class = "dijkstra">
				<div class = "text">Dijkstra's Cat vs Q1 mouse - <b>States (S)</b></div>
				For the sake of simplicity, I will use an 8 by 8 map grid for calculations and explanations but the actual game was done in a 15 by 15 map grid. All the concepts and MDPs still apply regardless of size. Here we are trying to define all the possible states that Q1 mouse can find itself when learning to evade Dijkstra's cat and escape the maze. <br><br>
				<ul class = "shift">
					<li><b>The set of coordinates the mouse can be in (row = 0, col = 0)...(row = 7, col = 7) ==> Total = 8 * 8 - 19 (walls) = 45</b>. </li>
					<li><b>Directions in which Dijkstra's cat is coming from (up, down, left, right) ==> Total = 4</b></li>
					<li><b>Shortest distance from the cat to the mouse (1, 2,... , 14, 15) ==> Total = 15</b>.</li>
				</ul>
				<br>
				So at most we will have 45 * 4 * 15  = <b>2700 states</b>. But we can do better. First of all, the cat rarely can come from all 4 directions. For example, if the mouse is in <b>(row = 7, col = 6)</b>, the cat can only come from the <b>left or right</b>. So the number of states the mouse can find itself in at <b>(row = 7, col = 6)</b> would be 2 (direction cat is coming from) * 15 (distance the cat is coming from) = <b>30</b>. Lets say the mouse was in <b>(row = 2, col = 2)</b>. The cat can come from <b>left, right, down</b>. How far can the cat be away? The black grid of values shows us the maximum distance the mouse can be from the cat at any point in the maze. At <b>(row = 2, col = 2)</b>, the cat can be at most <b>13</b> blocks away from the mouse. Therefore we have 3 * 13 states = <b>39</b>. After optimization, the number of total states we can have is <b>1295 states</b>.
			</div>
			<div class = "state1_image">
				<img src = "images/state1.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "dijkstra_info">
			<div class = "dijkstra">
				<div class = "text">Dijkstra's Cat vs Q1 mouse - <b>Actions (A) and Rewards (R)</b></div>
				The actions the mouse can take are pretty simple: <b>{up, down, left, right}</b>. If it takes an action against a wall, it will remain in the same block.<br><br>
				To design the reward function, we offer a positive reward for desired actions and negative rewards for undesired ones. The <b>maximum distance (15)</b> which is the <b>maximum distance the cat can be from the mouse is used as a benchmark value for offering rewards</b> - this is also for convenience when we enlarge the size of the map later. One important observation we can make here is that the <b>distance between the cat and mouse CANNOT increase</b> - this is due to the nature of Dijkstra's algorithm which always takes the shortest path towards the target. So the <b>distance between the cat and mouse</b> can either <b>stay the same or decrease</b>. The reward function is shown in the dark blue table. In general, we provide the mouse with a <b>positive reward</b> for <b>maintaining the distance</b> between itself and the cat. We also consider whether the mouse gets closer to the exit - a greater positive reward is provided if the mouse gets closer to the exit. The <b>ONLY</b> time we provide a <b>positive reward</b> to the mouse for <b>getting closer to the cat</b> is if the <b>mouse can reach the exit before the cat</b>. We deter the mouse for getting closer to the cat by applying a <b>negative reward</b> which is <b>-(max distance - distance from cat to mouse)</b>. If the cat is <b>only 2 blocks away</b>, the reward will be <b>-(15 - 3) = -12</b>. If the cat is <b>14 blocks away</b>, the reward will be <b>-(15 - 14) = -1</b>. This ensures the penalty is more severe as the cat grows closer and not as severe when the cat is far away.
			</div>
			<div class = "state1_image">
				<img src = "images/reward1.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "dijkstra_info">
			<div class = "dijkstra">
				<div class = "text">Epsilon <b>(Œµ)</b> Greedy Strategy- Exploration vs Exploitation </div>
				The agent often has to make the trade off between <b>exploration and exploitation</b>. <b>Exploration</b> involves trying out new actions to discover potentially greater rewards in unexplored states, while <b>exploitation</b> focuses on using past experiences to take what is believed to be the best action. The problem with the agent <b>exploiting</b> the situation is that it might overlook better opportunities that have not yet been explored. <br><br>

				For example, the mouse might know that turning right yields a reward of 10. If it never explores, it may never discover that turning left could yield a reward of 15. It's like if a child had the option of choosing between eating chicken, brocolli and wagyu steak. Lets say the child had always been eating chicken which provides a reward of 5. There are two brocollis on top of the wagyu steak so the child will have to eat both the brocollis first. Suppose the child eats the brocolli instead of the chicken and it is gross giving a reward of -5 so the child goes back to eating chicken. Now the child would never try wagyu steak which provides a reward of 20. To approach this problem, we employ the <b>Œµ greedy strategy</b> where we force the mouse to take a random action every once in a while.<br><br>
				The <b>Œµ greedy strategy</b> will involve <b>Œµ and a decay rate</b>. For every episode - which is a training session, <b>Œµ</b> will be updated by multiplying itself with the <b>decay rate</b>. Each time the mouse is to take an action, its probability of taking the best action is <b>1 - Œµ</b> and the probability of taking a random action is <b>Œµ</b>.
			</div>
			<div class = "state1_image">
				<img src = "images/epsilon.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "dijkstra_info">
			<div class = "dijkstra">
				<div class = "text">Q-learning process and algorithm</div>
				The Q-learning update equation is shown below:
				<br><br>
				<strong>\( Q(s, a) \leftarrow (1 - \alpha) \cdot Q(s, a) + \alpha \cdot [r + \gamma \cdot \max_{a'} Q(s', a')] \)</strong><br><br>
				<ul class = "shift">
					<li>\( Q(s, a)\): Estimated Q-values (the value of taking each action) given the current state</li>
					<li>\(\alpha \): The learning rate- how much the agent will take in new knowledge over old knowledge. A high \(\alpha \) means the agent learns quickly with new knowledge overriding past knowledge. A low \(\alpha \) means the agent will rely heaver on past knowledge. The learning will be slower but more stable.</li>
					<li>\(r\): The immediate reward for taking action \(a\) in state \(s\).</li>
					<li>\(\gamma\): The discount factor - represents the importance of future rewards.</li>
					<li>\(\max_{a'} Q(s', a')\): Maximum Q-value in the next state \(s'\) (the state the agent finds itself in as a result of taking action \(a\) in state \(s\)) across all possible next actions \(a'\). The best expected future reward.</li>
				</ul>
				<br>
				The steps of <b>updating the Q-table</b> (a table full of values which tell us how good it is to take an action \(a\) in state \(s\)) are as follows: The Q-table is first initialized to all zeros. Then the mouse takes an action \(a\) in state \(s\). As a result of taking action \(a\) in state \(s\), the mouse will transition to be in state \(s'\). The reward \(r\) will be calculated based on the conditions previously mentioned in <b>Actions (A) and Rewards (R)</b>. The value of the best action \(a'\) in the next state \(s'\) is found \(\max_{a'} Q(s', a')\). The value of \( Q(s, a)\) is updated based on the equation. Repeat - the mouse performs an action in this new state.
			</div>
			<div class = "state1_image">
				<img src = "images/qlearn.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "dijkstra_info">
			<div class = "dijkstra">
				<div class = "text">Example: choosing an action</div>
				In the diagram, we have two scenarios that we might see when the mouse is in <b>state(row = <mark>7</mark>, column = <mark>1</mark>, cat is coming from <mark>Left</mark>, cat is a distance of <mark>4</mark> away)</b> or we can write <b>(7, 1, L, 4)</b> for short. The corresponding Q-table is towards the right. The values filled in are hypothetical values you might find in a Q-table which has been updated several times already. Note that these values may not be to scale or accurate but its aimed at demonstrating how the Q-table works.<br><br>
				The first scenario towards the left, shows what might happen if the mouse is to take a random action in this state. Lets say the mouse randomly chose the action <b>up</b>. Our \( Q(s, a)\) here would be \( Q((7, 1, L, 4), up)\). We can look at the row highlighted in blue to see that performing this action would give us a <b>Q-value of -1.3</b> (which is not good). The best action would offer us a <b>Q-value of +3.7</b> which is to move <b>right</b>. The cat responds by moving downwards putting us in a new state \(s'\) (in orange) of <b>(7, 1, L, 3)</b> since the mouse hit a wall (did not move) and the cat moved one step closer.
				<br><br>
				The second scenario in the middle, shows what might happen if the mouse takes the best action in this state - which is <b>right</b>. The cat responds by moving downwards putting us in a new state \(s'\) (in purple) of <b>(7, 2, L, 4)</b>. 
			</div>
			<div class = "state1_image">
				<img src = "images/actions.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "dijkstra_info">
			<div class = "dijkstra">
				<div class = "text">Example continued: updating the Q-table</div>
				I usually use <b>0.95 for both \(\alpha \) and \(\gamma \)</b> - highlighted in pink and green.<br><br>
				Lets first take a look what happens if scenario 1 occurs. As a result of taking action \(a\) <b>up</b>, the mouse will find itself in the new state \(s'\) of <b>(7, 1, L, 3)</b>. The value of \(\max_{a'} Q(s', a')\) is the maximum Q-value of of the new state \(s'\) highlighted in orange. The maximum value is <b>+9.3</b> which is circled. The old value of \( Q(s, a)\) is <b>-1.3</b> which is the corresponding Q-value of the state action pair due to taking action \(a\) <b>up</b> in state <b>(7, 1, L, 4)</b>. To determine the reward, we can look back at the reward function previously defined. The reward that fits this scenario is the one where the mouse got closer to the cat. Therefore we apply the reward -(maximum distance - distance from cat to mouse) = -(15 - 3) = <b>-12</b>. Plug all these values into the update equation and update the value of \( Q(s, a)\) to the new calculated value of <b>-2.28</b>.<br><br>
				We approach it the exact same way if scenario 2 occurs. As a result of taking action \(a\) <b>right</b>, the mouse will find itself in the new state \(s'\) of <b>(7, 2, L, 4)</b>. The value of \(\max_{a'} Q(s', a')\) is <b>+16.5</b>. he old value of \( Q(s, a)\) is <b>+3.7</b> which is the corresponding Q-value of the state action pair due to taking action \(a\) <b>right</b> in state <b>(7, 1, L, 4)</b>. According to the reward function, we get a <b>reward of 15</b> because the mouse maintained its <b>distance of 4</b> from the cat and also got closer to the exit (from 13 to 12 steps). Plug all these values in and update the value of \( Q(s, a)\) to the new value of <b>+29.33</b>.
			</div>
			<br>
			<div class = "state1_image">
				<img src = "images/update.png" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "bottom_info">
			<div class = "initial_testing">
				<iframe width="560" height="315" src="https://www.youtube.com/embed/xLcG2L6hbgQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</div>
			<div class = "algorithm2">
				<div class = "text">Mouse Mechanics</div>
				The input push button values are read in from the GPIO port input data register and the mouse will then move towards the corresponding direction. Unlike the cat, the mouse's method of wall detection involved a more brute force approach. The challenge in this part was to ensure that not even a portion of the mouse would be able to overlap with a wall. Since both the mouse's and wall's location were only located at a single pixel coordinate, we could not just check if the two points were the same to detect wall collision obviously due to the fact that both the wall and mouse were larger than one pixel in size. Therefore each black wall tile had a "radius" of no entry to account for both the size of the wall and mouse. 
			</div>
		</div>
		<br>
		<div class = "additional_info">
			<div class = "algorithm3">
				<div class = "text">Map and Wall Generation</div>
				Apart from implementing the shortest path algorithm, one of the biggest problems faced was including several maps in the game. Towards the end, we were able to include 30+ hard coded maps. Because there was not enough flash memory to store 30 individual wallpaper backgrounds, we had to condense the information into a matrix data structure. The inner maze consisted of 14 by 10 tiles. However, we were able to further condense each map matrix into a 12 by 8 given that we will never place a wall directly next to the outer green blocks. With this done, we simply filled in each matrix coordinate with the appropriate binary value (eg. 0 for a path 1 for a wall) which was the minimum amount of information we needed in order to generate the maze. For each map chosen during gameplay, the selected matrix will be passed to a variety of functions in order to inform them of where a wall or path is. Using this method, the functions were able to determine where to place black/white block tiles, where to assign a length of 1 or 140 for the cat's algorithm and where to allow the mouse to go. 
			</div>
			<div class = "image1">
				<img src = "images/maze.jpg" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "spi_info">
			<div class = "spi">
				<div class = "text">SPI (Serial Peripheral Interface)</div>
				SPI was used to communicate between the microcontroller and the LCD display. In SPI communication, the two devices communicating are typically named <b>Master</b> and <b>Slave</b>. <b>Master</b> refers to the device which configures the clock speed, data format and other parameters for communication. The <b>Slave</b> refers to the other device which is able to receive information from the <b>Master</b> but (in some cases) also send back information to the Master. In this case, the microcontroller serves as Master and the LCD display serves as the slave.

				The SPI configuration usually consists of a 4 wire interface. <br><br>
				<ul class = "shift">
					<li><b>MOSI</b>(Master Out Slave In): </li> Bits of information are sent <b>OUT</b> from the <b>Master</b> and <b>IN</b> the <b>Slave</b>.
					<li><b>MISO</b>(Master In Slave Out): </li> Bits of information are sent <b>OUT</b> from the <b>Slave</b> and <b>IN</b> the <b>Master</b>.
					<li><b>SCLK</b>(Serial Clock or SCK): </li> Controlled by the Master to synchronize data transmission. Typically 1 bit is sent for every clock pulse. 
					<li><b>NSS</b>(Negative Slave Select or CS/Chip Select): Selects which slave to communicate with. Starts and ends communcation. The logic is active-low so when it is low, communication begins and when it is high, communcation ends.
				</ul>
				<br>
				On the example, we could see what it looks like to transfer 15 bits of information from the <b>Master</b> to the <b>Slave</b>. Communication only begins when the <b>NSS</b> is low and one bit is sent for every clock pulse. <br><br>
				Because the LCD display does not send back information to the microcontroller, we only use 3 of the 4 wires: <b>MOSI</b>, <b>SCK</b> and <b>NSS</b>. The Baud rate (or the frequency at which the bits are transferred) is controlled by the serial clock. In the STM32 microcontroller, we can configure the Baud rate in first SPI control register. The first SPI control register consists of 16 bits each with their own function. <br><br>
				<ul class = "shift">
					<li><b>SPI1->CR1 &= ~SPI_CR1_BR;</b></li> This line of code clears bits 3 to 5 which therefore selects the maximum Baud rate. fPCLK is 48 MHz for the STM32 microcontroller. As a result, we would be transferring bits at a rate of 48 MHz / 2 = 24 MHZ or 24 million bits per second. 
					<li><b>SPI1->CR1 |= SPI_CR1_MSTR;</b></li> This line of code sets bit 2 which therefore establishes the microcontroller as the <b>Master</b>. If this bit was cleared, the microcontroller would take on the role of <b>Slave</b>.
				</ul>
			</div>
			<div class = "image2">
				<img src = "images/spi.png" class = "pic" width = "100%" alt = "">
			</div>
			
			<div>

			</div>

		</div>


		<br>
		<div class = "pcb_design">
			<div class = "kicad">
				<img src = "images/pcb2.jpg" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "solder">
				<video width = "90%" height = "90%" controls>
					<source src = "videos/solder.mp4" type = "video/mp4">
				</video>
			</div>
			<div class = "pcb">
				<div class = "text">PCB design and soldering</div>
				On the hardware side, we drew the schematic and completed the layout using <b>Altium</b> from the ground up, referencing the Kicad design. We were able to utilize the huge online component library to quickly place component footprints and buy them online at the same time. The final PCB soldering process had some obstacles including being unable to reset the microcontroller. However, after some debugging, it turned out the issue was that we forgot to connect VCCA to 3V. After flywiring it to the 3V rail, the PCB worked as expected.
			</div>
		</div>
		<br>
		<div class = "pcb_design2">
			<div class = "pcb1">
				<img src = "images/pcb1.jpg" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pcb2">
				<img src = "images/printedboard.jpg" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "pcb3">
				<img src = "images/flywire.jpg" class = "pic" width = "100%" alt = "">
			</div>
		</div>
		<br>
		<div class = "final">
			<div class = "final1">
				<img src = "images/finalized1.jpg" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "final2">
				<img src = "images/finalized2.jpg" class = "pic" width = "100%" alt = "">
			</div>
			<div class = "final3">
				<img src = "images/finalized3.jpg" class = "pic" width = "100%" alt = "">
			</div>
		</div>
	</body>
</html>